#!/usr/bin/env python3
"""

UPGRADE 1 (Harder realism):
  - Systematics (timing bias + EOM loss) can depend on the *applied* phase phi_theta
    (not just raw theta). This makes holonomy detection harder/more realistic.

UPGRADE 2 (Path-based loops, not random labels):
  - CW/CCW loop label is generated by an explicit (u,v) control schedule.
  - Theta-bin is not iid random; it is walked along a closed loop in "control space"
    so CW vs CCW differ by traversal direction but share endpoints.

x_theta_simulation_v2.py
=========================================================
Fixes the v2 "stuck" issue by removing Python loops from permutation/bootstrap.


Key idea:
  1) Simulate trials as before (path-based CW/CCW schedule).
  2) Precompute per-block counts:
        C[block, theta, a, b, x, y]
     and store original block_loop[block] in {0,1}.
  3) Permutation: randomly flip blocks (0<->1) and combine counts using weights.
     No per-block Python loops. Fast on GPU.

Run:
  python x_theta_simulation_v2.py --N 2000000 --theta-bins 64 --systematics none --holonomy-amp 0.2 --use-gpu 1


run in order :

 Holonomy present:
  python x_theta_simulation_v2.py --N 2000000 --theta-bins 64 --systematics none --holonomy-amp 0.2 --use-gpu 1

Holonomy absent (should go null):
  python x_theta_simulation_v2.py --N 2000000 --theta-bins 64 --systematics none --holonomy-amp 0.0 --use-gpu 1

Hard systematics, holonomy absent (should still stay null-ish):
  python x_theta_simulation_v2.py --N 2000000 --theta-bins 64 --systematics hard --holonomy-amp 0.0 --use-gpu 1

Hard systematics + holonomy present (should survive):
  python x_theta_simulation_v2.py --N 2000000 --theta-bins 64 --systematics hard --holonomy-amp 0.2 --use-gpu 1
"""

import argparse
import math
import os
from dataclasses import dataclass
from typing import Dict, Tuple

import numpy as np
import torch


# -----------------------------
# Utilities
# -----------------------------


def pick_device(use_gpu: int) -> torch.device:
    if use_gpu and torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")


def set_seed(seed: int):
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


# -----------------------------
# Config
# -----------------------------


@dataclass
class SimConfig:
    N: int = 1_000_000
    theta_bins: int = 64
    seed: int = 123
    use_gpu: int = 1

    # stats
    perm_rounds: int = 400
    bootstrap_rounds: int = 300

    # physics
    vis: float = 0.98
    a0: float = 0.0
    a1: float = math.pi / 4
    b0: float = math.pi / 8
    b1: float = -math.pi / 8

    # theta model
    theta_amp: float = 0.7
    theta_freq: int = 1
    holonomy_amp: float = 0.20
    holonomy_harm: int = 1

    # systematics preset
    systematics: str = "none"  # none|mild|hard

    # coincidence / timing toy
    coincidence_window: float = 3.0e-9
    time_jitter_sigma: float = 0.8e-9
    timing_drift_amp: float = 0.0

    # settings RNG memory
    rng_memory: float = 0.0

    # efficiencies/bias
    eom_loss_amp: float = 0.0
    basis_eff_amp: float = 0.0
    local_bias_amp: float = 0.0

    # v2 realism knobs
    phi_coupled_jitter_scale: float = 0.5
    phi_coupled_loss: int = 1


def apply_systematics_preset(cfg: SimConfig) -> SimConfig:
    if cfg.systematics == "none":
        return cfg
    if cfg.systematics == "mild":
        cfg.rng_memory = 0.05
        cfg.eom_loss_amp = 0.03
        cfg.basis_eff_amp = 0.03
        cfg.timing_drift_amp = 0.4e-9
        cfg.local_bias_amp = 0.01
        return cfg
    if cfg.systematics == "hard":
        cfg.rng_memory = 0.15
        cfg.eom_loss_amp = 0.08
        cfg.basis_eff_amp = 0.08
        cfg.timing_drift_amp = 1.2e-9
        cfg.local_bias_amp = 0.03
        return cfg
    raise ValueError(cfg.systematics)


# -----------------------------
# Schedule (path-based)
# -----------------------------


def generate_theta_grid(T: int, device: torch.device) -> torch.Tensor:
    return torch.linspace(0, 2 * math.pi, T, device=device, dtype=torch.float64)


def make_cycle_indices(T: int, device: torch.device) -> torch.Tensor:
    forward = torch.arange(T, device=device, dtype=torch.int64)
    backward = torch.arange(T - 1, -1, -1, device=device, dtype=torch.int64)
    cycle = torch.cat([forward, backward[1:]], dim=0)  # length = 2T-1
    return cycle


def assign_trials_by_schedule(
    N: int, T: int, device: torch.device
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:
    """
    Returns:
      theta_bin[N], loop[N], block_id[N], n_blocks
    loop: 0=CCW, 1=CW
    """
    cycle = make_cycle_indices(T, device)
    L = int(cycle.numel())
    n_blocks = N // L
    N_use = n_blocks * L

    block_loop = (
        torch.arange(n_blocks, device=device, dtype=torch.int64) % 2
    )  # 0,1,0,1...
    loop = block_loop.repeat_interleave(L)
    block_id = torch.arange(
        n_blocks, device=device, dtype=torch.int64
    ).repeat_interleave(L)

    cycle_cw = cycle
    cycle_ccw = torch.flip(cycle, dims=[0])

    # Build theta assignment per block (vectorized via stacking then reshape)
    # blocks_theta[block, L]
    blocks_theta = torch.empty((n_blocks, L), device=device, dtype=torch.int64)
    blocks_theta[block_loop == 1] = cycle_cw
    blocks_theta[block_loop == 0] = cycle_ccw
    theta_bin = blocks_theta.reshape(-1)

    # Remainder -> append CW portion
    if N_use < N:
        rem = N - N_use
        extra = cycle_cw[:rem]
        theta_bin = torch.cat([theta_bin, extra], dim=0)
        loop = torch.cat(
            [loop, torch.ones((rem,), device=device, dtype=torch.int64)], dim=0
        )
        block_id = torch.cat(
            [block_id, torch.full((rem,), n_blocks, device=device, dtype=torch.int64)],
            dim=0,
        )
        n_blocks = n_blocks + 1  # last partial block

    return theta_bin, loop, block_id, n_blocks


# -----------------------------
# Random settings (with memory)
# -----------------------------


def generate_settings_with_memory(
    N: int, memory: float, device: torch.device
) -> Tuple[torch.Tensor, torch.Tensor]:
    a = torch.randint(0, 2, (N,), device=device, dtype=torch.int64)
    b = torch.randint(0, 2, (N,), device=device, dtype=torch.int64)
    if memory <= 0:
        return a, b

    u = torch.rand((N,), device=device, dtype=torch.float64)
    a_prev = torch.cat([a[:1], a[:-1]], dim=0)
    b_prev = torch.cat([b[:1], b[:-1]], dim=0)
    a = torch.where(u < memory, a_prev, a)
    b = torch.where(u < memory, b_prev, b)
    return a, b


# -----------------------------
# X–Θ model
# -----------------------------


def measurement_angles(
    a: torch.Tensor, b: torch.Tensor, cfg: SimConfig, device: torch.device
) -> Tuple[torch.Tensor, torch.Tensor]:
    alpha = torch.where(
        a == 0,
        torch.tensor(cfg.a0, device=device, dtype=torch.float64),
        torch.tensor(cfg.a1, device=device, dtype=torch.float64),
    )
    beta = torch.where(
        b == 0,
        torch.tensor(cfg.b0, device=device, dtype=torch.float64),
        torch.tensor(cfg.b1, device=device, dtype=torch.float64),
    )
    return alpha, beta


def x_theta_phase(
    theta: torch.Tensor, loop: torch.Tensor, cfg: SimConfig, device: torch.device
) -> torch.Tensor:
    base = cfg.theta_amp * torch.sin(cfg.theta_freq * theta)
    hol = cfg.holonomy_amp * torch.sin(cfg.holonomy_harm * theta)
    sign = torch.where(
        loop == 1,
        torch.tensor(1.0, device=device, dtype=torch.float64),
        torch.tensor(-1.0, device=device, dtype=torch.float64),
    )
    return base + sign * hol


# -----------------------------
# Simulate outcomes
# -----------------------------


def simulate_trials(
    theta_bin: torch.Tensor,
    loop: torch.Tensor,
    block_id: torch.Tensor,
    a: torch.Tensor,
    b: torch.Tensor,
    cfg: SimConfig,
    theta_grid: torch.Tensor,
    device: torch.device,
) -> Dict[str, torch.Tensor]:
    N = a.numel()
    theta = theta_grid[theta_bin]
    alpha, beta = measurement_angles(a, b, cfg, device)
    phi = x_theta_phase(theta, loop, cfg, device)

    E = -cfg.vis * torch.cos(2.0 * (alpha - beta + phi))

    x = torch.where(
        torch.rand((N,), device=device, dtype=torch.float64) < 0.5,
        torch.tensor(1, device=device, dtype=torch.int8),
        torch.tensor(-1, device=device, dtype=torch.int8),
    )
    p_same = (1.0 + E) / 2.0
    same = torch.rand((N,), device=device, dtype=torch.float64) < p_same
    y = torch.where(same, x, -x).to(torch.int8)

    if cfg.local_bias_amp > 0:
        bias = cfg.local_bias_amp * torch.sin(
            theta + 0.3 * a.to(torch.float64) - 0.2 * b.to(torch.float64)
        )
        p_flip_x = torch.clamp(0.5 * (bias + cfg.local_bias_amp), 0.0, 1.0)
        p_flip_y = torch.clamp(0.5 * (-bias + cfg.local_bias_amp), 0.0, 1.0)
        flip_x = torch.rand((N,), device=device, dtype=torch.float64) < p_flip_x
        flip_y = torch.rand((N,), device=device, dtype=torch.float64) < p_flip_y
        x = torch.where(flip_x, -x, x)
        y = torch.where(flip_y, -y, y)

    # efficiency
    detected = torch.ones((N,), device=device, dtype=torch.bool)
    if cfg.basis_eff_amp > 0 or cfg.eom_loss_amp > 0:
        eta0 = 0.92
        basis_term = cfg.basis_eff_amp * (
            (a.to(torch.float64) - 0.5) + (b.to(torch.float64) - 0.5)
        )
        loss_term = cfg.eom_loss_amp * torch.sin(phi if cfg.phi_coupled_loss else theta)
        eta = torch.clamp(eta0 + basis_term - loss_term, 0.02, 0.999)
        detected = torch.rand((N,), device=device, dtype=torch.float64) < eta

    # timing/coincidence
    tA = torch.randn((N,), device=device, dtype=torch.float64) * cfg.time_jitter_sigma
    tB = torch.randn((N,), device=device, dtype=torch.float64) * cfg.time_jitter_sigma

    if cfg.timing_drift_amp > 0:
        idx = torch.arange(N, device=device, dtype=torch.float64)
        drift = cfg.timing_drift_amp * torch.sin(2 * math.pi * idx / max(N, 1))
        tB = tB + drift

    jitter_bias = 0.4e-9 * torch.sin(
        theta
        + 0.8 * a.to(torch.float64)
        - 0.6 * b.to(torch.float64)
        + cfg.phi_coupled_jitter_scale * phi
    )
    tB = tB + jitter_bias

    coinc = torch.abs(tA - tB) < cfg.coincidence_window
    keep = detected & coinc

    return dict(
        theta_bin=theta_bin, loop=loop, block_id=block_id, a=a, b=b, x=x, y=y, keep=keep
    )


# -----------------------------
# Precompute per-block counts C[block, theta, a, b, x, y]
# -----------------------------


def trials_to_block_counts(
    data: Dict[str, torch.Tensor], T: int, n_blocks: int, device: torch.device
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Returns:
      C[block, theta, a, b, x, y] int64
      block_loop[block] in {0,1} (original)
    """
    keep = data["keep"]
    th = data["theta_bin"][keep].to(torch.int64)
    a = data["a"][keep].to(torch.int64)
    b = data["b"][keep].to(torch.int64)
    x = data["x"][keep].to(torch.int64)
    y = data["y"][keep].to(torch.int64)
    blk = data["block_id"][keep].to(torch.int64)
    loop = data["loop"][keep].to(torch.int64)

    # per kept event, map x,y to 0/1
    xi = torch.where(
        x == 1, torch.tensor(0, device=device), torch.tensor(1, device=device)
    )
    yi = torch.where(
        y == 1, torch.tensor(0, device=device), torch.tensor(1, device=device)
    )

    # Flatten index into [block, theta, a, b, x, y]
    # size = n_blocks * T * 2 * 2 * 2 * 2 = n_blocks * T * 16
    flat_size = n_blocks * T * 16
    idx = ((((blk * T + th) * 2 + a) * 2 + b) * 2 + xi) * 2 + yi

    Cflat = torch.zeros((flat_size,), device=device, dtype=torch.int64)
    Cflat.scatter_add_(0, idx, torch.ones_like(idx, dtype=torch.int64))
    C = Cflat.view(n_blocks, T, 2, 2, 2, 2)

    # Determine original block loop label by majority vote among kept events in each block
    # (works even if some events dropped)
    # If a block has zero kept events, fallback to scheduled label from data (use first occurrence).
    # We'll compute scheduled label via first index in block in original arrays.
    block_loop = torch.zeros((n_blocks,), device=device, dtype=torch.int64)
    # compute count of kept events with loop=1 per block
    ones = torch.ones_like(loop, dtype=torch.int64)
    num1 = torch.zeros((n_blocks,), device=device, dtype=torch.int64).scatter_add_(
        0, blk, (loop * ones)
    )
    num = torch.zeros((n_blocks,), device=device, dtype=torch.int64).scatter_add_(
        0, blk, ones
    )
    block_loop = torch.where(num > 0, (num1 * 2 >= num).to(torch.int64), block_loop)

    return C, block_loop


# -----------------------------
# Counts -> E,S
# -----------------------------


def counts_to_E_S(
    C: torch.Tensor, eps: float = 1e-12
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Input:
      C[theta, a, b, x, y] or C[loop, theta, a, b, x, y]
    Output:
      E[..., theta, a, b]
      S[..., theta]
      N_ab[..., theta, a, b]
    """
    N_ab = C.sum(dim=(-1, -2)).to(torch.float64)
    Npp = C[..., 0, 0].to(torch.float64)
    Npm = C[..., 0, 1].to(torch.float64)
    Nmp = C[..., 1, 0].to(torch.float64)
    Nmm = C[..., 1, 1].to(torch.float64)
    E = (Npp + Nmm - Npm - Nmp) / torch.clamp(N_ab, min=eps)
    S = E[..., 0, 0] + E[..., 0, 1] + E[..., 1, 0] - E[..., 1, 1]
    return E, S, N_ab


def approx_se_S(E: torch.Tensor, N_ab: torch.Tensor) -> torch.Tensor:
    seE = torch.sqrt(torch.clamp(1.0 - E**2, min=0.0) / torch.clamp(N_ab, min=1.0))
    v = (
        seE[..., 0, 0] ** 2
        + seE[..., 0, 1] ** 2
        + seE[..., 1, 0] ** 2
        + seE[..., 1, 1] ** 2
    )
    return torch.sqrt(v)


# -----------------------------
# Fast permutation / bootstrap using block weights
# -----------------------------


def combine_blocks(
    Cblk: torch.Tensor, block_loop: torch.Tensor, flips: torch.Tensor
) -> torch.Tensor:
    """
    Cblk: [B, T,2,2,2,2] int64
    Returns: [2, T,2,2,2,2] int64
    """
    eff = block_loop ^ flips.to(torch.int64)  # [B] 0/1
    w1 = eff.to(torch.float32)  # float weights
    w0 = 1.0 - w1

    # IMPORTANT: cast counts to float for einsum on CUDA
    C = Cblk.to(torch.float32)

    out0 = torch.einsum("b,btijkl->tijkl", w0, C)
    out1 = torch.einsum("b,btijkl->tijkl", w1, C)

    # Back to int64 (exact because weights are 0/1)
    return torch.stack(
        [out0.round().to(torch.int64), out1.round().to(torch.int64)], dim=0
    )


def permutation_test_fast(
    Cblk: torch.Tensor, block_loop: torch.Tensor, cfg: SimConfig, device: torch.device
) -> Tuple[torch.Tensor, float]:
    # observed (no flips)
    flips0 = torch.zeros_like(block_loop, dtype=torch.bool, device=device)
    Ctot = combine_blocks(Cblk, block_loop, flips0)
    E, S, N_ab = counts_to_E_S(Ctot)
    deltaS = S[1] - S[0]
    stat_obs = torch.max(torch.abs(deltaS))

    stats = []
    B = block_loop.numel()
    for _ in range(cfg.perm_rounds):
        flips = torch.rand((B,), device=device) < 0.5
        Cperm = combine_blocks(Cblk, block_loop, flips)
        _, Sperm, _ = counts_to_E_S(Cperm)
        d = Sperm[1] - Sperm[0]
        stats.append(torch.max(torch.abs(d)))
    stats = torch.stack(stats)
    p = (torch.sum(stats >= stat_obs) + 1).item() / (cfg.perm_rounds + 1)
    return deltaS, p


def bootstrap_fast(
    Cblk: torch.Tensor, block_loop: torch.Tensor, cfg: SimConfig, device: torch.device
) -> torch.Tensor:
    B = block_loop.numel()
    C = Cblk.to(torch.float32)  # cast once

    samples = []
    bl = block_loop.to(torch.float32)

    for _ in range(cfg.bootstrap_rounds):
        idx = torch.randint(0, B, (B,), device=device)
        w = torch.bincount(idx, minlength=B).to(torch.float32)

        w1 = w * bl
        w0 = w * (1.0 - bl)

        out0 = torch.einsum("b,btijkl->tijkl", w0, C)
        out1 = torch.einsum("b,btijkl->tijkl", w1, C)

        Ctot = torch.stack([out0, out1], dim=0)

        _, S, _ = counts_to_E_S(
            Ctot
        )  # counts_to_E_S already converts to float64 internally
        samples.append((S[1] - S[0]).unsqueeze(0))

    return torch.cat(samples, dim=0)


# -----------------------------
# FFT
# -----------------------------


def fft_spectrum(y: np.ndarray) -> np.ndarray:
    Y = np.fft.rfft(y - np.mean(y))
    return np.abs(Y)


# -----------------------------
# Main
# -----------------------------


def run(cfg: SimConfig):
    cfg = apply_systematics_preset(cfg)
    set_seed(cfg.seed)
    device = pick_device(cfg.use_gpu)

    print(f"[device] {device}")
    print(f"[cfg] N={cfg.N:,} T={cfg.theta_bins} systematics={cfg.systematics}")
    print(
        f"[X–Θ] theta_amp={cfg.theta_amp} holonomy_amp={cfg.holonomy_amp} vis={cfg.vis}"
    )
    print(
        f"[realism] phi_coupled_loss={cfg.phi_coupled_loss} phi_jitter_scale={cfg.phi_coupled_jitter_scale}"
    )

    theta_grid = generate_theta_grid(cfg.theta_bins, device)

    # schedule
    theta_bin, loop, block_id, n_blocks = assign_trials_by_schedule(
        cfg.N, cfg.theta_bins, device
    )

    # settings
    a, b = generate_settings_with_memory(cfg.N, cfg.rng_memory, device)

    # simulate
    data = simulate_trials(theta_bin, loop, block_id, a, b, cfg, theta_grid, device)
    kept = int(data["keep"].sum().item())

    # precompute per-block counts
    Cblk, block_loop = trials_to_block_counts(data, cfg.theta_bins, n_blocks, device)

    # observed totals (no permutation)
    Ctot = combine_blocks(
        Cblk, block_loop, torch.zeros_like(block_loop, dtype=torch.bool, device=device)
    )
    E, S, N_ab = counts_to_E_S(Ctot)
    seS = approx_se_S(E, N_ab)
    deltaS = S[1] - S[0]
    se_deltaS = torch.sqrt(seS[1] ** 2 + seS[0] ** 2)

    # weighted S over theta
    N_theta = N_ab.sum(dim=(-1, -2))
    w = torch.clamp(N_theta, min=1.0)
    S_weighted = (S * w).sum(dim=1) / w.sum(dim=1)
    dS_weighted = (S_weighted[1] - S_weighted[0]).item()

    print("\n[headline]")
    print(f"  kept trials: {kept:,} / {cfg.N:,}")
    print(f"  S_weighted CCW: {S_weighted[0].item(): .4f}")
    print(f"  S_weighted  CW: {S_weighted[1].item(): .4f}")
    print(f"  ΔS_weighted (CW-CCW): {dS_weighted: .4f}")

    # fast permutation
    obs_deltaS, p_perm = permutation_test_fast(Cblk, block_loop, cfg, device)
    stat_obs = torch.max(torch.abs(obs_deltaS)).item()
    print("\n[permutation test: FAST block flips]")
    print(f"  stat = max_theta |ΔS(θ)| = {stat_obs:.5f}")
    print(f"  p-value = {p_perm}")

    # fast bootstrap
    boot = bootstrap_fast(Cblk, block_loop, cfg, device)
    boot_np = boot.detach().cpu().numpy()
    deltaS_np = deltaS.detach().cpu().numpy()
    se_boot = np.std(boot_np, axis=0)

    # FFT
    spec_obs = fft_spectrum(deltaS_np)
    print("\n[FFT]")
    kmax = int(np.argmax(spec_obs[1:]) + 1) if spec_obs.size > 1 else 0
    print(f"  dominant harmonic (excluding DC) k={kmax}")

    # save
    os.makedirs("out", exist_ok=True)
    out_path = os.path.join(
        "out", f"x_theta_v3_{cfg.systematics}_N{cfg.N}_T{cfg.theta_bins}.npz"
    )
    np.savez(
        out_path,
        theta_grid=theta_grid.detach().cpu().numpy(),
        E=E.detach().cpu().numpy(),
        S=S.detach().cpu().numpy(),
        seS=seS.detach().cpu().numpy(),
        deltaS=deltaS_np,
        se_deltaS_approx=se_deltaS.detach().cpu().numpy(),
        se_deltaS_boot=se_boot,
        p_perm=p_perm,
        spec_obs=spec_obs,
        kept=kept,
        n_blocks=n_blocks,
        cfg=vars(cfg),
    )
    print(f"\n[saved] {out_path}")

    print("\n[sample θ bins]")
    for k in np.linspace(0, cfg.theta_bins - 1, num=min(8, cfg.theta_bins), dtype=int):
        print(
            f"  k={k:3d} θ={float(theta_grid[k].item()):.3f}  ΔS={deltaS_np[k]: .5f}  SE_boot={se_boot[k]:.5f}"
        )


def parse_args() -> SimConfig:
    p = argparse.ArgumentParser()
    p.add_argument("--N", type=int, default=2_000_000)
    p.add_argument("--theta-bins", type=int, default=64)
    p.add_argument("--seed", type=int, default=123)
    p.add_argument("--use-gpu", type=int, default=1)

    p.add_argument("--perm-rounds", type=int, default=400)
    p.add_argument("--bootstrap-rounds", type=int, default=300)

    p.add_argument(
        "--systematics", type=str, default="none", choices=["none", "mild", "hard"]
    )

    p.add_argument("--vis", type=float, default=0.98)
    p.add_argument("--theta-amp", type=float, default=0.7)
    p.add_argument("--theta-freq", type=int, default=1)
    p.add_argument("--holonomy-amp", type=float, default=0.20)
    p.add_argument("--holonomy-harm", type=int, default=1)

    p.add_argument("--phi-jitter-scale", type=float, default=0.5)
    p.add_argument("--phi-coupled-loss", type=int, default=1, choices=[0, 1])

    args = p.parse_args()
    return SimConfig(
        N=args.N,
        theta_bins=args.theta_bins,
        seed=args.seed,
        use_gpu=args.use_gpu,
        perm_rounds=args.perm_rounds,
        bootstrap_rounds=args.bootstrap_rounds,
        systematics=args.systematics,
        vis=args.vis,
        theta_amp=args.theta_amp,
        theta_freq=args.theta_freq,
        holonomy_amp=args.holonomy_amp,
        holonomy_harm=args.holonomy_harm,
        phi_coupled_jitter_scale=args.phi_jitter_scale,
        phi_coupled_loss=args.phi_coupled_loss,
    )


if __name__ == "__main__":
    cfg = parse_args()
    run(cfg)
